Different data formats -- to say nothing of different object models and APIs -- provide different affordances and present various advantages and disadvantages for their users and consumers. This much is obvious, but the specifics (*which* data format, object model or interface to the data is best for which situation or context) is muddled, with many opinions, many myths and guesses ("X is lightweight", "Y is 'self-describing'", "Everyone will be using Z"), and little that can be demonstrated or proven. This is both "fractal", and continuous with culture or the larger marketplace: Everyone is correct, about their own small patch of ground. But we are also wrong, because we are over-generalizing: over in the next field, things are the-same-but-different. This makes decisions about which sort of design will be good for which patch of ground -- for ourselves and everyone who lives there -- both consequential, and difficult.

But it is not just the terrain that is fractal, it is the technologies themselves that exhibit some interesting fractal properties. There is self-similarity across scales. Issues that arise at one level of the system have weird echoes elsewhere. Despite large-scale success (the "easy 80%") we never fully resolve things around the edges. (Until someone else does. For their use case.) We are constantly thinking about granularity and granularity mismatches, both semantic and "merely syntax" (and what's the difference) -- at different levels of granularity. Indeed, one way of discriminating between both tried-and-true, and up-and-coming data representations and serializations -- XML, HTML, Markdown, JSON, YAML -- is to distinguish between their different approaches to the problem of managing the chaos and representing (ir)regularity. One benefit of this examination is a better understanding of how to exploit their differences to make them work better together.
